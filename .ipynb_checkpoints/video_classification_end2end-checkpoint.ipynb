{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"pytorchvideo\")\n",
    "from pytorchvideo.data import LabeledVideoDataset,Kinetics, make_clip_sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/datasets/mohamedmustafa/real-life-violence-situations-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:04.995677Z",
     "iopub.status.busy": "2022-09-15T20:18:04.995237Z",
     "iopub.status.idle": "2022-09-15T20:18:06.789586Z",
     "shell.execute_reply": "2022-09-15T20:18:06.788360Z",
     "shell.execute_reply.started": "2022-09-15T20:18:04.995639Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pytorch_lightning import seed_everything, LightningModule, Trainer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor\n",
    "from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau,CosineAnnealingWarmRestarts,OneCycleLR,CosineAnnealingLR\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset,ConcatDataset,default_collate\n",
    "from sklearn.model_selection import KFold,GroupShuffleSplit,GroupKFold,LeaveOneGroupOut\n",
    "from torchmetrics import MeanAbsoluteError\n",
    "from sklearn.utils import shuffle\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchaudio import transforms as TA\n",
    "from sklearn.metrics import classification_report\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:06.794066Z",
     "iopub.status.busy": "2022-09-15T20:18:06.792689Z",
     "iopub.status.idle": "2022-09-15T20:18:06.803825Z",
     "shell.execute_reply": "2022-09-15T20:18:06.802108Z",
     "shell.execute_reply.started": "2022-09-15T20:18:06.794021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu118'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "non=glob('Normal/*')\n",
    "vio=glob('Shoplifting/*')\n",
    "label=[0]*len(non)+[1]*len(vio)\n",
    "df=pd.DataFrame(zip(non+vio,label),columns=['file','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,val_df=train_test_split(df,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:06.872186Z",
     "iopub.status.busy": "2022-09-15T20:18:06.871763Z",
     "iopub.status.idle": "2022-09-15T20:18:06.889484Z",
     "shell.execute_reply": "2022-09-15T20:18:06.888400Z",
     "shell.execute_reply.started": "2022-09-15T20:18:06.872133Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorchvideo.data import LabeledVideoDataset,Kinetics, make_clip_sampler\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "#     RemoveKey,\n",
    "#     ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    Permute\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize\n",
    ")\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:06.892088Z",
     "iopub.status.busy": "2022-09-15T20:18:06.891199Z",
     "iopub.status.idle": "2022-09-15T20:18:06.897433Z",
     "shell.execute_reply": "2022-09-15T20:18:06.896290Z",
     "shell.execute_reply.started": "2022-09-15T20:18:06.892051Z"
    }
   },
   "outputs": [],
   "source": [
    "#tuneable params\n",
    "num_video_samples=20\n",
    "video_duration=2\n",
    "model_name='efficient_x3d_xs'\n",
    "batch_size=8\n",
    "scheduler='cosine'\n",
    "clipmode='random'\n",
    "img_size=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:06.901622Z",
     "iopub.status.busy": "2022-09-15T20:18:06.900753Z",
     "iopub.status.idle": "2022-09-15T20:18:06.910916Z",
     "shell.execute_reply": "2022-09-15T20:18:06.909562Z",
     "shell.execute_reply.started": "2022-09-15T20:18:06.901576Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler,labeled_video_dataset\n",
    "from torchvision.transforms import ColorJitter,RandomAdjustSharpness,RandomAutocontrast\n",
    "video_transform = Compose(\n",
    "            [\n",
    "            ApplyTransformToKey(\n",
    "              key=\"video\",\n",
    "              transform=Compose(\n",
    "                  [\n",
    "                    UniformTemporalSubsample(num_video_samples),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                      #Determines the shorter spatial dim of the video (i.e. width or height) and scales it to the given size\n",
    "                    RandomShortSideScale(min_size=img_size+16, max_size=img_size+32),\n",
    "                    CenterCropVideo(img_size),\n",
    "                    RandomHorizontalFlip(p=0.5),\n",
    "                  ]\n",
    "                ),\n",
    "              ),\n",
    "            ]\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:06.924208Z",
     "iopub.status.busy": "2022-09-15T20:18:06.923758Z",
     "iopub.status.idle": "2022-09-15T20:18:06.958966Z",
     "shell.execute_reply": "2022-09-15T20:18:06.955103Z",
     "shell.execute_reply.started": "2022-09-15T20:18:06.924171Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset=labeled_video_dataset(val_df,\n",
    "                   clip_sampler=make_clip_sampler(clipmode, video_duration),\\\n",
    "                    transform=video_transform, decode_audio=False\n",
    "                                   )\n",
    "        \n",
    "train_loader=DataLoader(train_dataset,batch_size=4,\n",
    "           num_workers=0,\n",
    "           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:06.971415Z",
     "iopub.status.busy": "2022-09-15T20:18:06.970920Z",
     "iopub.status.idle": "2022-09-15T20:18:12.962845Z",
     "shell.execute_reply": "2022-09-15T20:18:12.961746Z",
     "shell.execute_reply.started": "2022-09-15T20:18:06.971365Z"
    }
   },
   "outputs": [],
   "source": [
    "batch=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:12.964898Z",
     "iopub.status.busy": "2022-09-15T20:18:12.964470Z",
     "iopub.status.idle": "2022-09-15T20:18:12.972822Z",
     "shell.execute_reply": "2022-09-15T20:18:12.971638Z",
     "shell.execute_reply.started": "2022-09-15T20:18:12.964831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 20, 224, 224]), torch.Size([4, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['video'].shape,batch['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T20:18:12.975981Z",
     "iopub.status.busy": "2022-09-15T20:18:12.975072Z",
     "iopub.status.idle": "2022-09-15T20:18:13.003089Z",
     "shell.execute_reply": "2022-09-15T20:18:13.001856Z",
     "shell.execute_reply.started": "2022-09-15T20:18:12.975944Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "class OurModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(OurModel,self).__init__()\n",
    "\n",
    "        self.scheduler=scheduler\n",
    "\n",
    "        \n",
    "        self.video_model = torch.hub.load('facebookresearch/pytorchvideo', model_name, pretrained=True)\n",
    "        self.video_model.projection.model=nn.Linear(in_features=2048, out_features=1000, bias=True)\n",
    "        \n",
    "       \n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear=nn.Linear(1000,1)\n",
    "        \n",
    "        self.lr=1e-3\n",
    "        self.batch_size=batch_size\n",
    "        self.numworker=6\n",
    "        \n",
    "        self.metric = torchmetrics.Accuracy(task='binary')\n",
    "        self.criterion=nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self,video):\n",
    "        x=self.video_model(video)\n",
    "        x=self.relu(x)\n",
    "        x=self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt=torch.optim.AdamW(params=self.parameters(),lr=self.lr )\n",
    "        if self.scheduler=='cosine':\n",
    "            scheduler=CosineAnnealingLR(opt,T_max=10,  eta_min=1e-6, last_epoch=-1)\n",
    "            return {'optimizer': opt,'lr_scheduler':scheduler}\n",
    "        elif self.scheduler=='reduce':\n",
    "            scheduler=ReduceLROnPlateau(opt,mode='min', factor=0.5, patience=5)\n",
    "            return {'optimizer': opt,'lr_scheduler':scheduler,'monitor':'val_loss'}\n",
    "        elif self.scheduler=='warm':\n",
    "            scheduler=CosineAnnealingWarmRestarts(opt,T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1)\n",
    "            return {'optimizer': opt,'lr_scheduler':scheduler}\n",
    "        elif self.scheduler=='cycle':\n",
    "            opt=torch.optim.AdamW(params=self.parameters(),lr=1e-6 )\n",
    "            scheduler=OneCycleLR(opt,max_lr=1e-2,epochs=15,steps_per_epoch=len(self.train_df)//self.batch_size//4)\n",
    "            lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n",
    "            return {'optimizer': opt, 'lr_scheduler': lr_scheduler}\n",
    "        elif self.scheduler=='lambda':\n",
    "            lambda1 = lambda epoch: 0.9 ** epoch\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda1)\n",
    "            return {'optimizer': opt, 'lr_scheduler': scheduler}\n",
    "        elif self.scheduler=='constant':\n",
    "            return opt\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        dataset=labeled_video_dataset(train_df,\n",
    "                   clip_sampler=make_clip_sampler(clipmode, video_duration),\\\n",
    "                    transform=video_transform, decode_audio=False)\n",
    "        \n",
    "        loader=DataLoader(dataset,batch_size=self.batch_size,\n",
    "                   num_workers=self.numworker,\n",
    "                   pin_memory=True)\n",
    "        return loader\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        video,label=batch['video'],batch['label']\n",
    "#         label=label.ravel().to(torch.int64)\n",
    "        out = self(video)\n",
    "        loss=self.criterion(out,label)\n",
    "        metric=self.metric(out,label.to(torch.int64))\n",
    "        return {'loss':loss,'metric':metric.detach()}\n",
    "\n",
    "    def on_train_epoch_end(self, outputs):\n",
    "        loss=torch.stack([x[\"loss\"] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        metric=torch.stack([x[\"metric\"] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        self.log('train_loss', loss,batch_size=self.batch_size)\n",
    "        self.log('train_metric', metric,batch_size=self.batch_size)\n",
    "        print('training loss ',self.current_epoch,loss,metric)\n",
    "   \n",
    "    def val_dataloader(self):\n",
    "        dataset=labeled_video_dataset(val_df,\n",
    "                   clip_sampler=make_clip_sampler(clipmode, video_duration),\\\n",
    "                    transform=video_transform, decode_audio=False)\n",
    "        \n",
    "        loader=DataLoader(dataset,batch_size=self.batch_size,\n",
    "                   num_workers=self.numworker,\n",
    "                   pin_memory=True)\n",
    "        return loader\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        video,label=batch['video'],batch['label']\n",
    "        out = self(video)\n",
    "        loss=self.criterion(out,label)\n",
    "        metric=self.metric(out,label.to(torch.int64))\n",
    "        return {'loss':loss,'metric':metric.detach()}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss=torch.stack([x[\"loss\"] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        metric=torch.stack([x[\"metric\"] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        print('validation loss ',self.current_epoch,loss,metric)\n",
    "        self.log('val_loss', loss,batch_size=self.batch_size)\n",
    "        self.log('val_metric',metric,batch_size=self.batch_size)\n",
    "   \n",
    "    def test_dataloader(self):\n",
    "        dataset=labeled_video_dataset(val_df,\n",
    "                   clip_sampler=make_clip_sampler(clipmode, video_duration),\\\n",
    "                    transform=video_transform, decode_audio=False)\n",
    "        \n",
    "        loader=DataLoader(dataset,batch_size=self.batch_size,\n",
    "                   num_workers=self.numworker,\n",
    "                   pin_memory=True)\n",
    "        return loader\n",
    "\n",
    "  \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        video,label=batch['video'],batch['label']\n",
    "        out = self(video)\n",
    "        return { 'label': label.detach(), 'pred': out.detach()}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        label = torch.cat([x['label'] for x in outputs]).cpu().numpy()\n",
    "        pred = torch.cat([x['pred'] for x in outputs]).cpu().numpy()\n",
    "        pred=np.where(pred>0.5,1,0)\n",
    "        print(classification_report(label, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-15T20:18:13.004769Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss',dirpath='checkpoints',\n",
    "                                        filename='file',save_last=True)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(project=\"violence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\user/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n",
      "Global seed set to 0\n",
      "c:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\lightning_fabric\\connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "model=OurModel()\n",
    "seed_everything(0)\n",
    "trainer = Trainer(max_epochs=30, \n",
    "#                 deterministic=True,\n",
    "                accelerator='gpu', devices=-1,\n",
    "                  precision=16,\n",
    "                accumulate_grad_batches=2,\n",
    "                enable_progress_bar = False,\n",
    "                num_sanity_val_steps=0,\n",
    "                  callbacks=[lr_monitor,checkpoint_callback],\n",
    "#                 limit_train_batches=5,\n",
    "#                 limit_val_batches=1,\n",
    "#                 logger=wandb_logger\n",
    "\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Support for `training_epoch_end` has been removed in v2.0.0. `OurModel` implements this method. You can use the `on_train_epoch_end` hook instead. To access outputs, save them in-memory as instance attributes. You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model,\n\u001b[0;32m      2\u001b[0m \u001b[39m#             ckpt_path='checkpoints/last.ckpt'\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m            )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:529\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    527\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    528\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 529\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    530\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    531\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:568\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[0;32m    559\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[0;32m    560\u001b[0m )\n\u001b[0;32m    562\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    563\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    564\u001b[0m     ckpt_path,\n\u001b[0;32m    565\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    566\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    567\u001b[0m )\n\u001b[1;32m--> 568\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    570\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    571\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:921\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector\u001b[39m.\u001b[39m_attach_model_callbacks()\n\u001b[0;32m    919\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector\u001b[39m.\u001b[39m_attach_model_logging_functions()\n\u001b[1;32m--> 921\u001b[0m _verify_loop_configurations(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    923\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[0;32m    924\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: preparing data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:36\u001b[0m, in \u001b[0;36m_verify_loop_configurations\u001b[1;34m(trainer)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected: Trainer state fn must be set before validating loop configuration.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mFITTING:\n\u001b[1;32m---> 36\u001b[0m     __verify_train_val_loop_configuration(trainer, model)\n\u001b[0;32m     37\u001b[0m     __verify_manual_optimization_support(trainer, model)\n\u001b[0;32m     38\u001b[0m     __check_training_step_requires_dataloader_iter(model)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:77\u001b[0m, in \u001b[0;36m__verify_train_val_loop_configuration\u001b[1;34m(trainer, model)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39m# check legacy hooks are not present\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mgetattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mtraining_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)):\n\u001b[1;32m---> 77\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSupport for `training_epoch_end` has been removed in v2.0.0. `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(model)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` implements this\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m method. You can use the `on_train_epoch_end` hook instead. To access outputs, save them in-memory as\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     80\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m instance attributes.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[0;32m     83\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mgetattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mvalidation_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m     84\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m     85\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSupport for `validation_epoch_end` has been removed in v2.0.0. `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(model)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` implements this\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m method. You can use the `on_validation_epoch_end` hook instead. To access outputs, save them in-memory as\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m instance attributes.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Support for `training_epoch_end` has been removed in v2.0.0. `OurModel` implements this method. You can use the `on_train_epoch_end` hook instead. To access outputs, save them in-memory as instance attributes. You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520."
     ]
    }
   ],
   "source": [
    "trainer.fit(model,\n",
    "#             ckpt_path='checkpoints/last.ckpt'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_res=trainer.validate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger.experiment.save('notebook.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
